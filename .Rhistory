# function that computes and stores the simulated statistics using T_stat
statistics <- function(theta) {
s_matrix <- s(theta)
t_vec <- T_stat(s_matrix, s_matrix, theta)
}
# alternative objective function
new_objective <- function(theta) {
t_values <- statistics(theta)
t_obs <- t_values[1]
# sort the R+1 t_statistics in increasing order
t_values <- sort(t_values)
obj <- t_values[floor(alpha * (R + 1)) + 1] - t_obs
return(obj)
}
# pick the midpoint if t_init is not specified
if (is.null(t_init)) {
t_init <- (lower_bds + upper_bds)/2
}
# minimize the objective function and see if it can be negative
opt <- optim(par = t_init,
fn = new_objective,
method = "L-BFGS-B",
lower = lower_bds,
upper = upper_bds)
opt_value <- opt$value
theta_hat <- opt$par
results <- list(opt_val = opt_value,
theta_hat = theta_hat)
return(results)
}
# helper function for the alternative accept function
accept2_helper <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
# extract the number of seeds R
R <- dim(seeds)[1]
d <- length(s_obs)
# a function that generate R simulated values using the seeds and G, store s_obs and s_sim in an R+1 by d matrix
s <- function(theta) {
s_values <- rbind(s_obs, G(seeds, theta))
return(s_values)
}
# function that computes and stores the simulated statistics using T_stat
statistics <- function(theta) {
s_matrix <- s(theta)
t_vec <- T_stat(s_matrix, s_matrix, theta)
}
# alternative objective function
new_objective <- function(theta) {
t_values <- statistics(theta)
t_obs <- t_values[1]
# sort the R+1 t_statistics in increasing order
t_values <- sort(t_values)
obj <- t_values[floor(alpha * (R + 1)) + 1] - t_obs
return(obj)
}
# pick the midpoint if t_init is not specified
if (is.null(t_init)) {
t_init <- (lower_bds + upper_bds)/2
}
# minimize the objective function and see if it can be negative
opt <- optim(par = t_init,
fn = new_objective,
method = "L-BFGS-B",
lower = lower_bds,
upper = upper_bds)
opt_value <- opt$value
theta_hat <- opt$par
results <- list(opt_val = opt_value,
theta_hat = theta_hat)
return(results)
}
# accept2 function
accept2 <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
optim_value <- accept2_helper(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth)$opt_val
return(optim_value < 0)
}
# get_CI with modified objective
get_CI2 <- function(alpha, lower_bds, upper_bds, j, seeds, G, s_obs, tol, t_init = NULL, T_stat = ma_depth) {
# j indicates that we're computing the confidence interval for the jth parameter
# tol represents the allowed tolerance on the boundary of our interval
# T_stat is default to ma_depth as defined in the p_val file
# use the p_value function to identify the best starting point for bisection, if there exits any
general_search <- accept2_helper(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init, T_stat)
if (general_search$opt_val < 0) {
# use the jth coordinate of theta_hat as the starting point for bisection
beta_init <- general_search$theta_hat[j]
# define the sub_search function which returns whether or not a given interval contains a valid point
sub_search <- function(beta_left, beta_right) {
updated_lower <- lower_bds
updated_lower[j] <- beta_left
updated_upper <- upper_bds
updated_upper[j] <- beta_right
# call the accept function to see if (beta_left, beta_right) contains a valid point
return(accept2(alpha, updated_lower, updated_upper, seeds, G, s_obs, NULL, T_stat))
}
# bisection to find the left boundary
bisect_left <- function(beta_left, beta_right) {
left <- beta_left
right <- beta_right
right_previous <- beta_right
while (right-left > tol/2) {
if (sub_search(left, right)) {
right_previous <- right
right <- (left + right)/2
} else {
left <- right
right <- (right + right_previous)/2
}
}
return(left)
}
# bisection to find the right boundary
bisect_right <- function(beta_left, beta_right) {
left <- beta_left
left_previous <- beta_left
right <- beta_right
while (right-left > tol/2) {
if (sub_search(left, right)) {
left_previous <- left
left <- (left + right)/2
} else {
right <- left
left <- (left_previous + left)/2
}
}
return(right)
}
# use bisect_left and bisect_right to compute the left/right boundaries
beta_L <- bisect_left(lower_bds[j], beta_init)
beta_R <- bisect_right(beta_init, upper_bds[j])
return(c(beta_L, beta_R))
} else {
return(NULL) # no point in the parameter space is likely, so return the empty set
}
}
get_CI(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
get_CI2(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
# helper function for the alternative accept function
accept2_helper <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
# extract the number of seeds R
R <- dim(seeds)[1]
d <- length(s_obs)
# a function that generate R simulated values using the seeds and G, store s_obs and s_sim in an R+1 by d matrix
s <- function(theta) {
s_values <- rbind(s_obs, G(seeds, theta))
return(s_values)
}
# function that computes and stores the simulated statistics using T_stat
statistics <- function(theta) {
s_matrix <- s(theta)
t_vec <- T_stat(s_matrix, s_matrix, theta)
}
# alternative objective function
new_objective <- function(theta) {
t_values <- statistics(theta)
t_obs <- t_values[1]
# sort the R+1 t_statistics in increasing order
t_values <- sort(t_values)
obj <- t_values[floor(alpha * (R + 1))] - t_obs
return(obj)
}
# pick the midpoint if t_init is not specified
if (is.null(t_init)) {
t_init <- (lower_bds + upper_bds)/2
}
# minimize the objective function and see if it can be negative
opt <- optim(par = t_init,
fn = new_objective,
method = "L-BFGS-B",
lower = lower_bds,
upper = upper_bds)
opt_value <- opt$value
theta_hat <- opt$par
results <- list(opt_val = opt_value,
theta_hat = theta_hat)
return(results)
}
# accept2 function
accept2 <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
optim_value <- accept2_helper(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth)$opt_val
return(optim_value < 0)
}
get_CI(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
get_CI2(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
# the Mahalanobis depth (modified to also take in a parameter theta)
# x: each row of x is a vector whose distance we want to compute,
# data: an (R+1) by d matrix containing (R+1) vectors
ma_depth <- function(x, data, theta) {
return(depth.Mahalanobis(x, data))
}
# test case for ma_depth
data_test <- matrix(data = rnorm(50),
nrow = 10,
ncol =5)
x_test <- data_test[1:2,]
ma_depth(x_test, data_test, 0)
# p_value function
p_value <- function(lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
# extract the number of seeds R
R <- dim(seeds)[1]
d <- length(s_obs)
# a function that generate R simulated values using the seeds and G, store s_obs and s_sim in an R+1 by d matrix
s <- function(theta) {
s_values <- rbind(s_obs, G(seeds, theta))
return(s_values)
}
# function that computes and stores the simulated statistics using T_stat
statistics <- function(theta) {
s_matrix <- s(theta)
t_vec <- T_stat(s_matrix, s_matrix, theta)
}
# define the counting function that we feed into optim
count <- function(theta) {
t_values <- statistics(theta)
ct <- sum(t_values[-1] <= t_values[1]) + t_values[1]
return(-ct)
}
# pick the midpoint if t_init is not specified
if (is.null(t_init)) {
t_init <- (lower_bds + upper_bds)/2
}
# finding the largest
opt <- optim(par = t_init,
fn = count,
method = "L-BFGS-B",
lower = lower_bds,
upper = upper_bds)
m <- -opt$value
theta_hat <- opt$par
# compute the p value and return
p_val <- 1/(R+1) * min(floor(m)+1, R+1)
# compile a list of values to return
results <- list(p_val = p_val,
rank = floor(m)+1,
theta_hat = theta_hat)
return(results)
}
# accept function
accept <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
# calling the p_value function as a subroutine
p_val <- p_value(lower_bds, upper_bds, seeds, G, s_obs, t_init, T_stat)$p_val
# return true if we fail to reject
return(p_val > alpha)
}
# helper function for the alternative accept function
accept2_helper <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
# extract the number of seeds R
R <- dim(seeds)[1]
d <- length(s_obs)
# a function that generate R simulated values using the seeds and G, store s_obs and s_sim in an R+1 by d matrix
s <- function(theta) {
s_values <- rbind(s_obs, G(seeds, theta))
return(s_values)
}
# function that computes and stores the simulated statistics using T_stat
statistics <- function(theta) {
s_matrix <- s(theta)
t_vec <- T_stat(s_matrix, s_matrix, theta)
}
# alternative objective function
new_objective <- function(theta) {
t_values <- statistics(theta)
t_obs <- t_values[1]
# sort the R+1 t_statistics in increasing order
t_values <- sort(t_values)
obj <- t_values[floor(alpha * (R + 1))] - t_obs
return(obj)
}
# pick the midpoint if t_init is not specified
if (is.null(t_init)) {
t_init <- (lower_bds + upper_bds)/2
}
# minimize the objective function and see if it can be negative
opt <- optim(par = t_init,
fn = new_objective,
method = "L-BFGS-B",
lower = lower_bds,
upper = upper_bds)
opt_value <- opt$value
theta_hat <- opt$par
results <- list(opt_val = opt_value,
theta_hat = theta_hat)
return(results)
}
# accept2 function
accept2 <- function(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth) {
optim_value <- accept2_helper(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init = NULL, T_stat = ma_depth)$opt_val
return(optim_value < 0)
}
source("p_val.R")
get_CI <- function(alpha, lower_bds, upper_bds, j, seeds, G, s_obs, tol, t_init = NULL, T_stat = ma_depth) {
# j indicates that we're computing the confidence interval for the jth parameter
# tol represents the allowed tolerance on the boundary of our interval
# T_stat is default to ma_depth as defined in the p_val file
# use the p_value function to identify the best starting point for bisection, if there exits any
general_search <- p_value(lower_bds, upper_bds, seeds, G, s_obs, t_init, T_stat)
if (general_search$p_val > alpha) {
# use the jth coordinate of theta_hat as the starting point for bisection
beta_init <- general_search$theta_hat[j]
# define the sub_search function which returns whether or not a given interval contains a valid point
sub_search <- function(beta_left, beta_right) {
updated_lower <- lower_bds
updated_lower[j] <- beta_left
updated_upper <- upper_bds
updated_upper[j] <- beta_right
# call the accept function to see if (beta_left, beta_right) contains a valid point
return(accept(alpha, updated_lower, updated_upper, seeds, G, s_obs, NULL, T_stat))
}
# bisection to find the left boundary
bisect_left <- function(beta_left, beta_right) {
left <- beta_left
right <- beta_right
right_previous <- beta_right
while (right-left > tol/2) {
if (sub_search(left, right)) {
right_previous <- right
right <- (left + right)/2
} else {
left <- right
right <- (right + right_previous)/2
}
}
return(left)
}
# bisection to find the right boundary
bisect_right <- function(beta_left, beta_right) {
left <- beta_left
left_previous <- beta_left
right <- beta_right
while (right-left > tol/2) {
if (sub_search(left, right)) {
left_previous <- left
left <- (left + right)/2
} else {
right <- left
left <- (left_previous + left)/2
}
}
return(right)
}
# use bisect_left and bisect_right to compute the left/right boundaries
beta_L <- bisect_left(lower_bds[j], beta_init)
beta_R <- bisect_right(beta_init, upper_bds[j])
return(c(beta_L, beta_R))
} else {
return(NULL) # no point in the parameter space is likely, so return the empty set
}
}
# get_CI with modified objective
get_CI2 <- function(alpha, lower_bds, upper_bds, j, seeds, G, s_obs, tol, t_init = NULL, T_stat = ma_depth) {
# j indicates that we're computing the confidence interval for the jth parameter
# tol represents the allowed tolerance on the boundary of our interval
# T_stat is default to ma_depth as defined in the p_val file
# use the p_value function to identify the best starting point for bisection, if there exits any
general_search <- accept2_helper(alpha, lower_bds, upper_bds, seeds, G, s_obs, t_init, T_stat)
if (general_search$opt_val < 0) {
# use the jth coordinate of theta_hat as the starting point for bisection
beta_init <- general_search$theta_hat[j]
# define the sub_search function which returns whether or not a given interval contains a valid point
sub_search <- function(beta_left, beta_right) {
updated_lower <- lower_bds
updated_lower[j] <- beta_left
updated_upper <- upper_bds
updated_upper[j] <- beta_right
# call the accept function to see if (beta_left, beta_right) contains a valid point
return(accept2(alpha, updated_lower, updated_upper, seeds, G, s_obs, NULL, T_stat))
}
# bisection to find the left boundary
bisect_left <- function(beta_left, beta_right) {
left <- beta_left
right <- beta_right
right_previous <- beta_right
while (right-left > tol/2) {
if (sub_search(left, right)) {
right_previous <- right
right <- (left + right)/2
} else {
left <- right
right <- (right + right_previous)/2
}
}
return(left)
}
# bisection to find the right boundary
bisect_right <- function(beta_left, beta_right) {
left <- beta_left
left_previous <- beta_left
right <- beta_right
while (right-left > tol/2) {
if (sub_search(left, right)) {
left_previous <- left
left <- (left + right)/2
} else {
right <- left
left <- (left_previous + left)/2
}
}
return(right)
}
# use bisect_left and bisect_right to compute the left/right boundaries
beta_L <- bisect_left(lower_bds[j], beta_init)
beta_R <- bisect_right(beta_init, upper_bds[j])
return(c(beta_L, beta_R))
} else {
return(NULL) # no point in the parameter space is likely, so return the empty set
}
}
source("s_sample.R")
source("get_CI.R")
s_sample <- function(u, theta) {
# Extract the mean (location) and std. dev (scale) from theta
mean <- theta[1]
sd <- theta[2]
# Determine the number of samples and dimensions from the seed matrix u
# are determined from the `u` matrix.
num_samples <- nrow(u)
num_dimensions <- ncol(u)
# Initialize a matrix to store the samples
samples <- matrix(0, nrow = num_samples, ncol = num_dimensions)
# Generate samples from a location-scale normal distribution for each dimension
# loop iterates over each dimension, generating set of random nums. from a normal distribution
# w/  specified mean and std. dev.for that dimension.
# These nums are then added to the `samples` matrix.
for (i in  1 : num_samples) {
samples[i,] <- qnorm(u[i,], mean = mean, sd = sd/sqrt(100)) # input sample size
}
return(samples) # matrix of simulated samples returned
}
sample_size <- 100 # sample size
R <- 200 # Repro sample size R
alpha <- .05 # significance level
tol <- 10^-4
# population parameters
mu <- 1
sigma <- 1
seeds <- matrix(data = runif(R)) # 200 pre-generated seeds
# generate 50 normal samples based on the population mean and sd
data <- rnorm(n = sample_size,
mean = mu,
sd = sigma)
s_obs <- mean(data)
# unit testing
repro_data <- s_sample(seeds, c(mu, sigma))
plot(density(repro_data), lwd = 2)
p_value(c(-10, 0.999), c(-1, 1), seeds, s_sample, s_obs)
accept(alpha, c(-10, 0.999), c(-1, 1), seeds, s_sample, s_obs)
# this is the classical way of finding the confidence interval for mu
normal_CI <- function(data, alpha) {
z <- qnorm(1 - alpha/2)
x_bar <- mean(data)
sample_size <- length(data)
half_width <- z/sqrt(sample_size)
interval <- c(x_bar - half_width, x_bar + half_width)
return(interval)
}
classical_CI <- normal_CI(data, alpha)
# this is the confidence interval generated by Repro sampling
Repro_CI <- get_CI(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
Repro_CI
get_CI2(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
Repro_CI
get_CI2(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
s_sample <- function(u, theta) {
# Extract the mean (location) and std. dev (scale) from theta
mean <- theta[1]
sd <- theta[2]
# Determine the number of samples and dimensions from the seed matrix u
# are determined from the `u` matrix.
num_samples <- nrow(u)
num_dimensions <- ncol(u)
# Initialize a matrix to store the samples
samples <- matrix(0, nrow = num_samples, ncol = num_dimensions)
# Generate samples from a location-scale normal distribution for each dimension
# loop iterates over each dimension, generating set of random nums. from a normal distribution
# w/  specified mean and std. dev.for that dimension.
# These nums are then added to the `samples` matrix.
for (i in  1 : num_samples) {
samples[i,] <- qnorm(u[i,], mean = mean, sd = sd/sqrt(100)) # input sample size
}
return(samples) # matrix of simulated samples returned
}
sample_size <- 100 # sample size
R <- 200 # Repro sample size R
alpha <- .05 # significance level
tol <- 10^-4
# population parameters
mu <- 1
sigma <- 1
seeds <- matrix(data = runif(R)) # 200 pre-generated seeds
# generate 50 normal samples based on the population mean and sd
data <- rnorm(n = sample_size,
mean = mu,
sd = sigma)
s_obs <- mean(data)
# unit testing
repro_data <- s_sample(seeds, c(mu, sigma))
plot(density(repro_data), lwd = 2)
p_value(c(-10, 0.999), c(-1, 1), seeds, s_sample, s_obs)
accept(alpha, c(-10, 0.999), c(-1, 1), seeds, s_sample, s_obs)
# this is the classical way of finding the confidence interval for mu
normal_CI <- function(data, alpha) {
z <- qnorm(1 - alpha/2)
x_bar <- mean(data)
sample_size <- length(data)
half_width <- z/sqrt(sample_size)
interval <- c(x_bar - half_width, x_bar + half_width)
return(interval)
}
classical_CI <- normal_CI(data, alpha)
# this is the confidence interval generated by Repro sampling
Repro_CI <- get_CI(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
Repro_CI
get_CI2(alpha, c(-10, 0.999), c(10, 1), 1, seeds, s_sample, s_obs, tol)
